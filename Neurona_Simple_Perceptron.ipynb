{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jserrataylor/cursoAI/blob/main/Neurona_Simple_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptrón"
      ],
      "metadata": {
        "id": "SzUeQVCX7qW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **El Perceptrón: Una Introducción a las Redes Neuronales Artificiales**\n",
        "\n",
        "El perceptrón, creado en 1957 por Frank Rosenblatt, es un modelo fundamental en el aprendizaje automático y un bloque básico en las redes neuronales artificiales. Este algoritmo simple para el aprendizaje supervisado de clasificadores binarios marca el inicio en el estudio de la inteligencia artificial, jugando un papel crucial en la comprensión de conceptos más avanzados.\n",
        "\n",
        "![Grafica](https://miro.medium.com/v2/resize:fit:600/format:webp/0*k6nvli_G-bz3J2zB.jpg)\n",
        "\n",
        "## **Comparación con la Neurona Biológica**\n",
        "\n",
        "El perceptrón se asemeja a una neurona biológica en su estructura y función:\n",
        "\n",
        "- **Estructura:**\n",
        "  - Neurona Biológica: Compuesta por dendritas, cuerpo celular y axón.\n",
        "  - Perceptrón: Incluye entradas (similares a dendritas), pesos (eficacia sináptica), función de suma (integración de señales) y función de activación (generación de potencial de acción).\n",
        "\n",
        "- **Procesamiento de Información:**\n",
        "  - Neurona Biológica: Suma analógica de señales y generación de potencial de acción.\n",
        "  - Perceptrón: Suma ponderada de entradas y aplicación de una función de activación.\n",
        "\n",
        "- **Aprendizaje:**\n",
        "  - Neurona Biológica: Ajuste de conexiones sinápticas (plasticidad sináptica).\n",
        "  - Perceptrón: Ajuste de pesos según un algoritmo de aprendizaje para minimizar errores.\n",
        "\n",
        "- **Función y Limitaciones:**\n",
        "  - Neurona Biológica: Funciones variadas en un sistema biológico complejo.\n",
        "  - Perceptrón: Clasificación y predicción simples; limitado a problemas linealmente separables sin redes más complejas.\n",
        "\n",
        "![Gráfica](https://d2f0ora2gkri0g.cloudfront.net/dd/db/dddb807b-a15b-457d-a21a-8a9e6f029a3e.png)\n",
        "\n",
        "A pesar de sus diferencias, el perceptrón sigue siendo una herramienta poderosa, inspirada por la eficiencia y capacidad de procesamiento de las neuronas biológicas.\n",
        "\n",
        "## **Representación Esquemática del Perceptrón**\n",
        "\n",
        "Una representación típica del perceptrón incluye nodos de entrada, pesos, un sumador y una función de activación. Estas partes simulan el proceso de recepción, ponderación y procesamiento de señales en la neurona artificial.\n",
        "\n",
        "![Gráfica](https://www.alexisalulema.com/wp-content/uploads/2022/09/image-2-1536x808.png)\n",
        "\n",
        "## **Características Clave del Perceptrón**\n",
        "\n",
        "1. **Estructura Básica**: Entradas múltiples con pesos ajustables, suma ponderada y función de activación binaria.\n",
        "\n",
        "- **Nodos de entrada (x1, x2, x3, ..., xm)**: Estos círculos amarillos representan las neuronas de entrada, que reciben las señales a procesar. En este caso, hay m entradas, lo que sugiere que el perceptrón puede procesar m características diferentes.\n",
        "- **Pesos (w1, w2, w3, ..., wm)**: Los círculos color salmón a la derecha de las entradas representan los pesos asignados a cada señal de entrada. Estos pesos son los factores que se ajustan durante el proceso de aprendizaje de la red neuronal y determinan la importancia de cada entrada en la salida del perceptrón.\n",
        "- **Sumatoria (Σ)**: El círculo grande azul en el centro es el sumador. Su función es multiplicar cada entrada (xi) por su peso correspondiente (wi) y sumar todos estos productos. Esto se conoce como la suma ponderada de las entradas.\n",
        "- **Función de activación**: El cuadro rojo en el extremo derecho representa la función de activación que se aplica a la suma ponderada de las entradas. La línea negra dentro del cuadro rojo es un gráfico de una función escalón, que es una función de activación común en los perceptrones. Esta función convierte la suma ponderada en una salida binaria, generalmente 0 o 1, que se utiliza para la clasificación.\n",
        "\n",
        "2. **Proceso de Aprendizaje**: Entrenamiento con ejemplos y ajuste de pesos mediante la regla de aprendizaje del perceptrón.\n",
        "\n",
        "3. **Limitaciones**: Solo clasifica datos linealmente separables y no resuelve problemas no lineales.\n",
        "4. **Evolución y Uso**: Fundamento para redes neuronales más complejas y diversas aplicaciones en inteligencia artificial.\n",
        "\n",
        "## **Referencias**\n",
        "\n",
        "Sgantzos, K., Grigg, I., & Al Hemairy, M. (2022). Multiple Neighborhood Cellular Automata as a Mechanism for Creating an AGI on a Blockchain. Journal of Risk and Financial Management, 15(8). https://doi.org/10.3390/jrfm15080360"
      ],
      "metadata": {
        "id": "4nriH2tWqtkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[Demostración del entrenamiento de un Perceptrón en una Hoja de Cálculo de Google Sheets](https://docs.google.com/spreadsheets/d/1tSarA8ixoJvTtZiYbjf9D8QtuRNind-Ei3mVr6Xz-1w/edit#gid=0)**\n",
        "\n",
        "\n",
        "![Imagen Demostración de un **Perceptrón** en una Hoja de Cálculo](https://files.oaiusercontent.com/file-lWJ7Quv4trk2RCQldqCnCkze?se=2023-12-05T20%3A37%3A52Z&sp=r&sv=2021-08-06&sr=b&rscc=max-age%3D3599%2C%20immutable&rscd=attachment%3B%20filename%3Dimage.png&sig=3q9td8OgtLTLl3snvS3n7GPOw4oU9xAE%2Bx7xjAIzKk4%3D)\n"
      ],
      "metadata": {
        "id": "ZRHfVtYoae1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Pass en el Perceptrón:\n",
        "\n",
        "1. **Calculo de la Suma Ponderada**: En cada época, para cada conjunto de entradas $ X_1 $ y $ X_2 $, calculas la suma ponderada. Esta suma es la combinación lineal de las entradas y sus respectivos pesos más el sesgo:\n",
        "   $ Suma = X_1 \\cdot W1 + X_2 \\cdot W2 + Sesgo \\cdot W0 $\n",
        "\n",
        "2. **Aplicación de la Función de Activación**: Usas la función escalón para obtener el output $ F(X) $:\n",
        "\n",
        "  **1**  si la Suma >= 0\n",
        "  \n",
        "  **0**  si la Suma < 0\n",
        "\n",
        "  Esto convierte la salida del perceptrón en un valor binario (1 o 0).\n",
        "\n",
        "### Backward Pass en el Perceptrón:\n",
        "\n",
        "1. **Cálculo del Error**: Comparas la salida $ F(X) $ con la salida deseada (lo que quieres). El error es la diferencia entre estos dos valores.\n",
        "\n",
        "2. **Ajuste de los Pesos**: Basándote en este error, ajustas los pesos $ W0, W1, W2 $. Los cambios propuestos en los pesos $ Cambio W0, Cambio W1, Cambio W2 $ se calculan en función de este error y se aplican para mejorar la precisión del modelo en la próxima iteración. El ajuste se hace generalmente usando una regla de aprendizaje simple como la Regla de Aprendizaje del Perceptrón.\n",
        "\n",
        "   Por ejemplo, el ajuste del peso podría ser algo así:\n",
        "   $ W_{nuevo} = W_{viejo} + (Coef. Aprendizaje \\times Error \\times Entrada)$\n",
        "\n",
        "### Características del Perceptrón con Función de Activación Escalón:\n",
        "\n",
        "- **Modelo Linealmente Separable**: El perceptrón con función de activación escalón es efectivo solo si los datos son linealmente separables, ya que produce una frontera de decisión lineal.\n",
        "\n",
        "- **Actualizaciones de Peso Binarias**: Debido a la naturaleza de la función escalón, los ajustes de peso son binarios, basados en si hay un error o no, lo cual es diferente de modelos que usan funciones de activación como la Sigmoid o ReLU, donde los ajustes son más graduales.\n",
        "\n",
        "\n",
        "Este perceptrón realiza tanto un forward pass como un backward pass en cada iteración o época, utilizando la función de activación escalón para generar predicciones binarias y ajustando los pesos en función del error observado."
      ],
      "metadata": {
        "id": "hMzbYerO5lhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Propagación hacia Adelante (Forward Pass) y Retropropagación (Backward Pass)\n",
        "\n",
        "Para entender el forward y backward pass en el contexto de tu tabla, primero definamos qué representa cada columna:\n",
        "\n",
        "- **Sesgo (Sesgo)**: Valor constante añadido a la suma de las entradas y los pesos. Normalmente es 1.\n",
        "- **X1, X2**: Entradas de la red.\n",
        "- **W0, W1, W2**: Pesos asociados a cada entrada y al sesgo.\n",
        "- **Suma**: Sumatoria de las entradas multiplicadas por sus respectivos pesos más el sesgo.\n",
        "- **F(X)**: Función de activación aplicada a la suma.\n",
        "- **Lo Que Queremos**: El valor deseado o etiqueta real para la entrada dada.\n",
        "- **Cambio W0, Cambio W1, Cambio W2**: Ajustes que se deben realizar en los pesos.\n",
        "- **Coef. Aprendizaje**: Tasa de aprendizaje utilizada para ajustar los pesos.\n",
        "\n",
        "### Forward Pass (Propagación hacia Adelante)\n",
        "\n",
        "El forward pass involucra calcular la salida de la red basada en las entradas actuales y los pesos. Para cada fila de tu tabla:\n",
        "\n",
        "1. **Calcula la Suma**: Multiplica cada entrada por su peso correspondiente y súmalos junto con el sesgo.\n",
        "   - Por ejemplo, para la primera fila: $ Suma = 41 \\times W0 + 0 \\times W1 + 0 \\times W2 + Sesgo \\times 1 $\n",
        "\n",
        "2. **Aplica la Función de Activación (F(X))**: Esto podría ser una Sigmoid, ReLU, etc. En tu tabla, parece que se utiliza una función de activación que devuelve 1 siempre, lo que es inusual.\n",
        "\n",
        "3. **Compara con lo Que Queremos**: Esto sería el valor esperado o la etiqueta real para la entrada dada.\n",
        "\n",
        "### Backward Pass (Retropropagación)\n",
        "\n",
        "El backward pass ajusta los pesos basándose en el error (la diferencia entre la salida actual y la deseada).\n",
        "\n",
        "1. **Calcula el Error**: La diferencia entre \"Lo Que Queremos\" y \"F(X)\".\n",
        "\n",
        "2. **Ajusta los Pesos (Cambio W0, W1, W2)**: Se utiliza el gradiente del error con respecto a cada peso. Este cálculo depende de la función de pérdida y de la función de activación.\n",
        "   - Por ejemplo, para W0 en la primera fila, el ajuste sería: $ W0_{nuevo} = W0_{viejo} - Coef. Aprendizaje \\times Cambio W0 $.\n",
        "\n",
        "3. **Actualiza los Pesos**: Aplica los ajustes a los pesos para la siguiente época.\n",
        "\n",
        "En tu tabla, cada fila representa una instancia de entrenamiento en una época. Los \"Cambios W0, W1, W2\" parecen ser los gradientes calculados para ajustar los pesos, y el \"Coef. Aprendizaje\" es la tasa a la cual se aplican estos ajustes.\n",
        "\n",
        "Por último, tu tabla parece representar un aprendizaje muy simplificado y específico para un tipo particular de red neuronal, posiblemente una red con una sola neurona sin capas ocultas, dado que la función de activación siempre devuelve 1. En escenarios de aprendizaje profundo más complejos, estos procesos involucran cálculos más avanzados y se manejan de manera automatizada por bibliotecas especializadas como TensorFlow o PyTorch."
      ],
      "metadata": {
        "id": "tuhwXG6N2iSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicios demostrativo del entrenamiento de un Perceptrón**"
      ],
      "metadata": {
        "id": "j5NwZP-eZ-BJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcXoUJ8gg_I8",
        "outputId": "53e8f29a-45fb-40f0-ddd7-02ecc72bbea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1:\n",
            "Entrada: [0, 0] -> Pesos: [0.5, 1.0, 1.0] -> Error: -1\n",
            "Entrada: [1, 0] -> Pesos: [0.0, 0.5, 1.0] -> Error: -1\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: -1\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 2:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 3:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 4:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 5:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 6:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "class SimpleNeuron:\n",
        "\n",
        "    def __init__(self, learning_rate, initial_weights):\n",
        "        \"\"\"\n",
        "        Inicializa la neurona con una tasa de aprendizaje y pesos iniciales.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = initial_weights\n",
        "\n",
        "    def activate(self, weighted_sum):\n",
        "        \"\"\"\n",
        "        Función de activación: devuelve 1 si la suma ponderada es positiva y 0 en caso contrario.\n",
        "        \"\"\"\n",
        "        return 1 if weighted_sum > 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        Realiza una predicción basada en las entradas y los pesos actuales.\n",
        "        \"\"\"\n",
        "        weighted_sum = sum([i*w for i, w in zip(inputs, self.weights)])\n",
        "        return self.activate(weighted_sum)\n",
        "\n",
        "    def train(self, inputs, desired_output):\n",
        "        \"\"\"\n",
        "        Entrena la neurona ajustando los pesos basados en el error entre la salida deseada y la predicción.\n",
        "        \"\"\"\n",
        "        prediction = self.predict(inputs)\n",
        "        error = desired_output - prediction\n",
        "        self.weights = [w + self.learning_rate * error * i for w, i in zip(self.weights, inputs)]\n",
        "        return error, self.weights\n",
        "\n",
        "# Datos iniciales\n",
        "initial_weights = [1.0, 1.0, 1.0]\n",
        "learning_rate = 0.5\n",
        "inputs_outputs = [\n",
        "    ([1, 0, 0], 0),\n",
        "    ([1, 1, 0], 0),\n",
        "    ([1, 0, 1], 0),\n",
        "    ([1, 1, 1], 1)\n",
        "]\n",
        "\n",
        "# Crear una neurona\n",
        "neuron = SimpleNeuron(learning_rate, initial_weights)\n",
        "\n",
        "# Simular la evolución de la neurona durante varias épocas\n",
        "epochs = 6\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Época {epoch + 1}:\")\n",
        "    for inputs, desired_output in inputs_outputs:\n",
        "        error, new_weights = neuron.train(inputs, desired_output)\n",
        "        print(f\"Entrada: {inputs[1:]} -> Pesos: {new_weights} -> Error: {error}\")\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir la tasa de aprendizaje\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Inicializar los pesos, por ejemplo, a 1\n",
        "weights = np.array([1.0, 1.0, 1.0])  # W0 es el sesgo, W1 y W2 son los pesos\n",
        "\n",
        "# Definir la función de activación, usaremos una función escalón\n",
        "def activation_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Función para el entrenamiento del perceptrón\n",
        "def train_perceptron(inputs, targets, epochs):\n",
        "    global weights\n",
        "    for epoch in range(epochs):\n",
        "        epoch_errors = []\n",
        "        for input, target in zip(inputs, targets):\n",
        "            # Calcula la suma ponderada\n",
        "            weighted_sum = np.dot(input, weights[1:]) + weights[0]  # W0 es el sesgo\n",
        "            # Calcula la salida de la función de activación\n",
        "            output = activation_function(weighted_sum)\n",
        "            # Calcula el error\n",
        "            error = target - output\n",
        "            epoch_errors.append(error)\n",
        "            # Actualiza los pesos y el sesgo\n",
        "            weights[1:] += learning_rate * error * input\n",
        "            weights[0] += learning_rate * error\n",
        "        # Imprimir la información de la época\n",
        "        epoch_accuracy = (np.array(epoch_errors) == 0).mean()\n",
        "        print(f'Época {epoch+1} - Error: {np.sum(np.abs(epoch_errors))}, Precisión: {epoch_accuracy}')\n",
        "        # Almacenar métricas de la época\n",
        "        epoch_metrics.append((epoch+1, np.sum(np.abs(epoch_errors)), epoch_accuracy))\n",
        "\n",
        "    return epoch_metrics\n",
        "\n",
        "# Datos de entrenamiento\n",
        "inputs = np.array([\n",
        "    # X1, X2\n",
        "    [0, 0],\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [1, 1],\n",
        "])\n",
        "\n",
        "# Etiquetas objetivo (Lo que queremos [1])\n",
        "targets = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Lista para almacenar las métricas de cada época\n",
        "epoch_metrics = []\n",
        "\n",
        "# Entrenar el perceptrón\n",
        "metrics = train_perceptron(inputs, targets, epochs=6)\n",
        "\n",
        "# Imprimir los pesos finales y las métricas de cada época\n",
        "weights, metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtJiuaMAVzbo",
        "outputId": "00cd252f-e2a4-4180-cd6e-0acf188b9908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1 - Error: 3, Precisión: 0.25\n",
            "Época 2 - Error: 2, Precisión: 0.5\n",
            "Época 3 - Error: 3, Precisión: 0.25\n",
            "Época 4 - Error: 2, Precisión: 0.5\n",
            "Época 5 - Error: 1, Precisión: 0.75\n",
            "Época 6 - Error: 0, Precisión: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-1.5,  0.5,  1. ]),\n",
              " [(1, 3, 0.25),\n",
              "  (2, 2, 0.5),\n",
              "  (3, 3, 0.25),\n",
              "  (4, 2, 0.5),\n",
              "  (5, 1, 0.75),\n",
              "  (6, 0, 1.0)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Red Neuronal**\n",
        "\n",
        "![Gráfica](https://i0.wp.com/unaaldia.hispasec.com/wp-content/uploads/2021/07/nn-architecture.png?w=510&ssl=1)\n",
        "\n",
        "\n",
        "![Grafica](https://www.researchgate.net/profile/Juan-Vasquez-Gomez/publication/334974413/figure/fig4/AS:788645715922947@1565039199504/Figura-29-Ejemplo-de-una-red-neuronal-convolucional-para-clasificacion-Goodfellow-et.ppm)"
      ],
      "metadata": {
        "id": "lWPgqjvZumTD"
      }
    }
  ]
}