{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIFm/rIqIsTmzUU7Y4Z9Uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jserrataylor/cursoAI/blob/main/Tipos_de_Redes_Neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tipos de Redes Neuronales**"
      ],
      "metadata": {
        "id": "N9asZg1qZAa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zMdZY7Nhkxpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos de los tipos más comunes de redes neuronales y sus diferencias principales. Cada tipo de red neuronal es adecuado para diferentes tipos de problemas, basándose en la naturaleza de los datos y el tipo de tarea a realizar. Por ejemplo, las CNNs son ideales para datos visuales, mientras que las RNN y sus variantes son mejores para datos secuenciales como texto o series temporales.\n",
        "\n",
        "1. **Redes Neuronales Densas o Completamente Conectadas (Fully Connected Neural Networks)**\n",
        "   - **Descripción**: En estas redes, cada neurona en una capa está conectada a todas las neuronas en la capa siguiente.\n",
        "   - **Uso**: Son útiles para problemas de clasificación y regresión general, donde la relación entre las entradas y las salidas no depende de datos espaciales o temporales.\n",
        "   - **Diferencia**: No consideran la relación espacial o temporal entre las entradas.\n",
        "\n",
        "2. **Redes Neuronales Convolucionales (Convolutional Neural Networks, CNNs)**\n",
        "   - **Descripción**: Se especializan en procesar datos con una estructura en forma de rejilla, como imágenes.\n",
        "   - **Uso**: Son ideales para tareas de visión computacional como reconocimiento de imágenes, análisis de video y clasificación de imágenes.\n",
        "   - **Diferencia**: Utilizan capas convolucionales que aplican filtros para capturar características espaciales.\n",
        "\n",
        "3. **Redes Neuronales Recurrentes (Recurrent Neural Networks, RNNs)**\n",
        "   - **Descripción**: Diseñadas para trabajar con secuencias de datos, tienen conexiones recurrentes que permiten información persistente.\n",
        "   - **Uso**: Se utilizan para tareas como el procesamiento del lenguaje natural, reconocimiento de voz y análisis de series temporales.\n",
        "   - **Diferencia**: Capaces de manejar secuencias de datos y recordar información previa gracias a sus conexiones recurrentes.\n",
        "\n",
        "4. **Long Short-Term Memory Networks (LSTM)**\n",
        "   - **Descripción**: Una variante avanzada de las RNN, diseñada para evitar el problema del desvanecimiento del gradiente al procesar secuencias largas.\n",
        "   - **Uso**: Útiles en aplicaciones similares a las RNN, pero mejoran el rendimiento en secuencias largas.\n",
        "   - **Diferencia**: Incorporan celdas especiales que pueden mantener información durante períodos más largos y regular el flujo de información.\n",
        "\n",
        "5. **Redes Neuronales de Memoria a Corto Plazo (Short-Term Memory Networks, GRU)**\n",
        "   - **Descripción**: Similar a LSTM, pero con una estructura más simplificada.\n",
        "   - **Uso**: Procesamiento de secuencias donde se requiere eficiencia computacional.\n",
        "   - **Diferencia**: Ofrecen un balance entre el poder de modelado de LSTM y la eficiencia de las RNN tradicionales.\n",
        "\n",
        "6. **Autoencoders**\n",
        "   - **Descripción**: Redes diseñadas para aprender representaciones codificadas (comprimidas) de los datos de entrada.\n",
        "   - **Uso**: Utilizados para reducción de dimensionalidad, des-ruido de datos y generación de nuevas instancias de datos.\n",
        "   - **Diferencia**: Su objetivo es reproducir la entrada en la salida, aprendiendo una representación compacta en el proceso.\n",
        "\n",
        "7. **Redes Neuronales Generativas Antagónicas (Generative Adversarial Networks, GANs)**\n",
        "   - **Descripción**: Consisten en dos redes, una generativa y otra discriminativa, que se entrenan en conjunto.\n",
        "   - **Uso**: Creación de contenido nuevo (como imágenes, música, texto), mejoramiento de imágenes, y más.\n",
        "   - **Diferencia**: Utilizan un enfoque de aprendizaje adversario donde dos redes se entrenan en competición.\n",
        "\n",
        "8. **Redes Neuronales de Gráficos (Graph Neural Networks, GNNs)**\n",
        "   - **Descripción**: Diseñadas para trabajar con datos en forma de gráficos.\n",
        "   - **Uso**: Análisis de redes sociales, sistemas de recomendación, y análisis de estructuras moleculares.\n",
        "   - **Diferencia**: Capaces de manejar datos estructurados como gráficos, donde las relaciones entre los nodos son clave."
      ],
      "metadata": {
        "id": "QGaiPQZ3ZfKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Requerimientos de preinstalación**"
      ],
      "metadata": {
        "id": "QBdDdSahk3ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install accelerate\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "VggYM-S9c-rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Redes Neuronales - Transformer - Bert**"
      ],
      "metadata": {
        "id": "n6GhPUvK4k5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "# Generar 100 oraciones sintéticas con etiquetas de sentimiento\n",
        "data = {\n",
        "    'sentence': [\n",
        "        \"I love spending time with my family\",  # Positivo\n",
        "        \"I feel lost and alone\",  # Negativo\n",
        "        \"This beautiful scenery is breathtaking\",  # Positivo\n",
        "        \"This has been a terrible day\",  # Negativo\n",
        "        \"I feel great today\",  # Positivo\n",
        "        \"I'm worried about the future\",  # Negativo\n",
        "        \"The movie was a delightful experience\",  # Positivo\n",
        "        \"I feel sick and tired\",  # Negativo\n",
        "        \"I am so grateful for all the good in my life\",  # Positivo\n",
        "        \"I'm disappointed with the results\",  # Negativo\n",
        "        # ... (continuar alternando hasta completar 100)\n",
        "    ] + [\"This is a good day\" if i % 2 == 0 else \"This is a bad day\" for i in range(10, 100)],\n",
        "    'label': [1 if i % 2 == 0 else 0 for i in range(100)]\n",
        "}\n",
        "\n",
        "# Crear DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "true_labels = test_df['label'].tolist()\n",
        "\n",
        "# Cargar tokenizer y modelo de BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenizar los datos\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_encodings = tokenize_function(train_df['sentence'].tolist())\n",
        "test_encodings = tokenize_function(test_df['sentence'].tolist())\n",
        "\n",
        "# Clase para el conjunto de datos\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Convertir a formato PyTorch\n",
        "train_dataset = CustomDataset(train_encodings, train_df['label'].tolist())\n",
        "test_dataset = CustomDataset(test_encodings, true_labels)\n",
        "\n",
        "# Definir argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Inicializar el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer.train()\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Convertir logits a etiquetas predichas\n",
        "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Cálculo de métricas\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "7FvWuWTRhpMs",
        "outputId": "1123022b-7919-4f55-bdaa-193dc653cea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 05:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nuevas oraciones para predecir\n",
        "new_sentences = [\"I can't continue working here!\", \"This is a bad day!\", \"I'm sure this day is sad\"]\n",
        "\n",
        "# Tokenizar las nuevas oraciones\n",
        "new_encodings = tokenizer(new_sentences, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Crear un conjunto de datos para las nuevas oraciones\n",
        "new_dataset = CustomDataset(new_encodings, [0] * len(new_sentences))  # Etiquetas ficticias\n",
        "\n",
        "# Hacer predicciones\n",
        "new_predictions = trainer.predict(new_dataset)\n",
        "\n",
        "# Convertir logits a etiquetas predichas\n",
        "new_pred_labels = np.argmax(new_predictions.predictions, axis=1)\n",
        "\n",
        "# Imprimir las etiquetas predichas\n",
        "print(new_pred_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "scMWbwHdjtJ_",
        "outputId": "50661145-fa91-4f09-8ba0-d672fbfeb90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Redes Neuronales - Transformer - GPT-2 de OpenAI**"
      ],
      "metadata": {
        "id": "eVyYS6fu3c2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Cargar tokenizer y modelo de GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Asegurarse de que el tokenizer utilice el mismo token EOS que el modelo GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tus datos de texto\n",
        "texts = [\"<Había una vez en un pueblo lejano, un pequeño pero valiente guerrero. Este guerrero se embarcó en una aventura épica para salvar a su pueblo de un dragón feroz. Durante su viaje, enfrentó numerosos desafíos, pero nunca perdió la esperanza ni el coraje.>\", \"<El amanecer en la playa era un espectáculo impresionante. Los colores del cielo cambiaban de tonos suaves de rosa a naranjas brillantes, mientras el sol se elevaba lentamente sobre el horizonte del mar tranquilo. Las olas rompían suavemente en la orilla, creando una melodía calmante.>\", \"<A menudo me encuentro reflexionando sobre el significado de la vida. Creo que la felicidad se encuentra en las pequeñas cosas: una sonrisa genuina, una conversación sincera, o el cálido abrazo de un ser querido. Estos momentos son los que verdaderamente atesoramos.>\"]  # Reemplaza esto con tu conjunto de datos\n",
        "\n",
        "# Tokenizar los datos\n",
        "encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Clase para el conjunto de datos\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Las etiquetas son las mismas que input_ids para entrenamiento de lenguaje\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.encodings['input_ids'][idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "# Convertir a formato PyTorch\n",
        "dataset = TextDataset(encodings)\n",
        "\n",
        "# Definir argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "# Inicializar el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "1vgm116lhvOU",
        "outputId": "6a6c71ba-bd82-45fc-d40c-c2415be9a186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=4.912951151529948, metrics={'train_runtime': 31.3583, 'train_samples_per_second': 0.287, 'train_steps_per_second': 0.191, 'total_flos': 459302400000.0, 'train_loss': 4.912951151529948, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de entrada para comenzar la generación\n",
        "input_text = \"El día comenzó\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generar texto\n",
        "output_sequences = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=100,  # Longitud máxima del texto generado\n",
        "    num_return_sequences=1,  # Número de secuencias de texto a generar\n",
        "    no_repeat_ngram_size=2,  # Tamaño del n-gram para evitar repeticiones\n",
        "    temperature=1.0,  # Temperatura para la variabilidad del texto generado\n",
        "    top_k=50,  # Número de tokens de alta probabilidad a considerar para cada paso\n",
        "    top_p=0.95,  # Probabilidad acumulativa para seleccionar tokens de alta probabilidad\n",
        ")\n",
        "\n",
        "# Decodificar las secuencias de salida\n",
        "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in output_sequences]\n",
        "\n",
        "# Imprimir el texto generado\n",
        "for i, generated_text in enumerate(generated_texts):\n",
        "    print(f\"Generated Text {i + 1}: {generated_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mvrBIFUkPHo",
        "outputId": "8f77a2ae-28c0-4b01-c41b-21a5700b7d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: El día comenzó a la ciudad de la vida de los más.\n",
            "\n",
            "El cielo de una cualquierda, el ciencia de las mujeros, que el muy bueno, con el víctor.\n"
          ]
        }
      ]
    }
  ]
}