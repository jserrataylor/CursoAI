{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN28iWomU+f5t+PGGQp1iIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jserrataylor/cursoAI/blob/main/Neurona_Simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptrón"
      ],
      "metadata": {
        "id": "SzUeQVCX7qW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **El Perceptrón: Una Introducción a las Redes Neuronales Artificiales**\n",
        "\n",
        "El perceptrón, creado en 1957 por Frank Rosenblatt, es un modelo fundamental en el aprendizaje automático y un bloque básico en las redes neuronales artificiales. Este algoritmo simple para el aprendizaje supervisado de clasificadores binarios marca el inicio en el estudio de la inteligencia artificial, jugando un papel crucial en la comprensión de conceptos más avanzados.\n",
        "\n",
        "![Grafica](https://miro.medium.com/v2/resize:fit:600/format:webp/0*k6nvli_G-bz3J2zB.jpg)\n",
        "\n",
        "## **Comparación con la Neurona Biológica**\n",
        "\n",
        "El perceptrón se asemeja a una neurona biológica en su estructura y función:\n",
        "\n",
        "- **Estructura:**\n",
        "  - Neurona Biológica: Compuesta por dendritas, cuerpo celular y axón.\n",
        "  - Perceptrón: Incluye entradas (similares a dendritas), pesos (eficacia sináptica), función de suma (integración de señales) y función de activación (generación de potencial de acción).\n",
        "\n",
        "- **Procesamiento de Información:**\n",
        "  - Neurona Biológica: Suma analógica de señales y generación de potencial de acción.\n",
        "  - Perceptrón: Suma ponderada de entradas y aplicación de una función de activación.\n",
        "\n",
        "- **Aprendizaje:**\n",
        "  - Neurona Biológica: Ajuste de conexiones sinápticas (plasticidad sináptica).\n",
        "  - Perceptrón: Ajuste de pesos según un algoritmo de aprendizaje para minimizar errores.\n",
        "\n",
        "- **Función y Limitaciones:**\n",
        "  - Neurona Biológica: Funciones variadas en un sistema biológico complejo.\n",
        "  - Perceptrón: Clasificación y predicción simples; limitado a problemas linealmente separables sin redes más complejas.\n",
        "\n",
        "![Gráfica](https://d2f0ora2gkri0g.cloudfront.net/dd/db/dddb807b-a15b-457d-a21a-8a9e6f029a3e.png)\n",
        "\n",
        "A pesar de sus diferencias, el perceptrón sigue siendo una herramienta poderosa, inspirada por la eficiencia y capacidad de procesamiento de las neuronas biológicas.\n",
        "\n",
        "## **Representación Esquemática del Perceptrón**\n",
        "\n",
        "Una representación típica del perceptrón incluye nodos de entrada, pesos, un sumador y una función de activación. Estas partes simulan el proceso de recepción, ponderación y procesamiento de señales en la neurona artificial.\n",
        "\n",
        "![Gráfica](https://www.alexisalulema.com/wp-content/uploads/2022/09/image-2-1536x808.png)\n",
        "\n",
        "## **Características Clave del Perceptrón**\n",
        "\n",
        "1. **Estructura Básica**: Entradas múltiples con pesos ajustables, suma ponderada y función de activación binaria.\n",
        "\n",
        "- **Nodos de entrada (x1, x2, x3, ..., xm)**: Estos círculos amarillos representan las neuronas de entrada, que reciben las señales a procesar. En este caso, hay m entradas, lo que sugiere que el perceptrón puede procesar m características diferentes.\n",
        "- **Pesos (w1, w2, w3, ..., wm)**: Los círculos color salmón a la derecha de las entradas representan los pesos asignados a cada señal de entrada. Estos pesos son los factores que se ajustan durante el proceso de aprendizaje de la red neuronal y determinan la importancia de cada entrada en la salida del perceptrón.\n",
        "- **Sumatoria (Σ)**: El círculo grande azul en el centro es el sumador. Su función es multiplicar cada entrada (xi) por su peso correspondiente (wi) y sumar todos estos productos. Esto se conoce como la suma ponderada de las entradas.\n",
        "- **Función de activación**: El cuadro rojo en el extremo derecho representa la función de activación que se aplica a la suma ponderada de las entradas. La línea negra dentro del cuadro rojo es un gráfico de una función escalón, que es una función de activación común en los perceptrones. Esta función convierte la suma ponderada en una salida binaria, generalmente 0 o 1, que se utiliza para la clasificación.\n",
        "\n",
        "2. **Proceso de Aprendizaje**: Entrenamiento con ejemplos y ajuste de pesos mediante la regla de aprendizaje del perceptrón.\n",
        "\n",
        "3. **Limitaciones**: Solo clasifica datos linealmente separables y no resuelve problemas no lineales.\n",
        "4. **Evolución y Uso**: Fundamento para redes neuronales más complejas y diversas aplicaciones en inteligencia artificial.\n",
        "\n",
        "## **Referencias**\n",
        "\n",
        "Sgantzos, K., Grigg, I., & Al Hemairy, M. (2022). Multiple Neighborhood Cellular Automata as a Mechanism for Creating an AGI on a Blockchain. Journal of Risk and Financial Management, 15(8). https://doi.org/10.3390/jrfm15080360"
      ],
      "metadata": {
        "id": "4nriH2tWqtkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¿Cómo funciona un Perceptrón? - Demostración en una Hoja de Cálculo (Google Sheets)**\n",
        "\n",
        "[Demostración de un **Perceptrón** en una Hoja de Cálculo](https://docs.google.com/spreadsheets/d/1tSarA8ixoJvTtZiYbjf9D8QtuRNind-Ei3mVr6Xz-1w/edit?usp=sharing)"
      ],
      "metadata": {
        "id": "ZRHfVtYoae1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicios demostrativo del entrenamiento de un Perceptrón**"
      ],
      "metadata": {
        "id": "j5NwZP-eZ-BJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcXoUJ8gg_I8",
        "outputId": "53e8f29a-45fb-40f0-ddd7-02ecc72bbea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1:\n",
            "Entrada: [0, 0] -> Pesos: [0.5, 1.0, 1.0] -> Error: -1\n",
            "Entrada: [1, 0] -> Pesos: [0.0, 0.5, 1.0] -> Error: -1\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: -1\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 2:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 3:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 4:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 5:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n",
            "Época 6:\n",
            "Entrada: [0, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 0] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [0, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "Entrada: [1, 1] -> Pesos: [-0.5, 0.5, 0.5] -> Error: 0\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "class SimpleNeuron:\n",
        "\n",
        "    def __init__(self, learning_rate, initial_weights):\n",
        "        \"\"\"\n",
        "        Inicializa la neurona con una tasa de aprendizaje y pesos iniciales.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = initial_weights\n",
        "\n",
        "    def activate(self, weighted_sum):\n",
        "        \"\"\"\n",
        "        Función de activación: devuelve 1 si la suma ponderada es positiva y 0 en caso contrario.\n",
        "        \"\"\"\n",
        "        return 1 if weighted_sum > 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        Realiza una predicción basada en las entradas y los pesos actuales.\n",
        "        \"\"\"\n",
        "        weighted_sum = sum([i*w for i, w in zip(inputs, self.weights)])\n",
        "        return self.activate(weighted_sum)\n",
        "\n",
        "    def train(self, inputs, desired_output):\n",
        "        \"\"\"\n",
        "        Entrena la neurona ajustando los pesos basados en el error entre la salida deseada y la predicción.\n",
        "        \"\"\"\n",
        "        prediction = self.predict(inputs)\n",
        "        error = desired_output - prediction\n",
        "        self.weights = [w + self.learning_rate * error * i for w, i in zip(self.weights, inputs)]\n",
        "        return error, self.weights\n",
        "\n",
        "# Datos iniciales\n",
        "initial_weights = [1.0, 1.0, 1.0]\n",
        "learning_rate = 0.5\n",
        "inputs_outputs = [\n",
        "    ([1, 0, 0], 0),\n",
        "    ([1, 1, 0], 0),\n",
        "    ([1, 0, 1], 0),\n",
        "    ([1, 1, 1], 1)\n",
        "]\n",
        "\n",
        "# Crear una neurona\n",
        "neuron = SimpleNeuron(learning_rate, initial_weights)\n",
        "\n",
        "# Simular la evolución de la neurona durante varias épocas\n",
        "epochs = 6\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Época {epoch + 1}:\")\n",
        "    for inputs, desired_output in inputs_outputs:\n",
        "        error, new_weights = neuron.train(inputs, desired_output)\n",
        "        print(f\"Entrada: {inputs[1:]} -> Pesos: {new_weights} -> Error: {error}\")\n",
        "    print(\"------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir la tasa de aprendizaje\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Inicializar los pesos, por ejemplo, a 1\n",
        "weights = np.array([1.0, 1.0, 1.0])  # W0 es el sesgo, W1 y W2 son los pesos\n",
        "\n",
        "# Definir la función de activación, usaremos una función escalón\n",
        "def activation_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Función para el entrenamiento del perceptrón\n",
        "def train_perceptron(inputs, targets, epochs):\n",
        "    global weights\n",
        "    for epoch in range(epochs):\n",
        "        epoch_errors = []\n",
        "        for input, target in zip(inputs, targets):\n",
        "            # Calcula la suma ponderada\n",
        "            weighted_sum = np.dot(input, weights[1:]) + weights[0]  # W0 es el sesgo\n",
        "            # Calcula la salida de la función de activación\n",
        "            output = activation_function(weighted_sum)\n",
        "            # Calcula el error\n",
        "            error = target - output\n",
        "            epoch_errors.append(error)\n",
        "            # Actualiza los pesos y el sesgo\n",
        "            weights[1:] += learning_rate * error * input\n",
        "            weights[0] += learning_rate * error\n",
        "        # Imprimir la información de la época\n",
        "        epoch_accuracy = (np.array(epoch_errors) == 0).mean()\n",
        "        print(f'Época {epoch+1} - Error: {np.sum(np.abs(epoch_errors))}, Precisión: {epoch_accuracy}')\n",
        "        # Almacenar métricas de la época\n",
        "        epoch_metrics.append((epoch+1, np.sum(np.abs(epoch_errors)), epoch_accuracy))\n",
        "\n",
        "    return epoch_metrics\n",
        "\n",
        "# Datos de entrenamiento\n",
        "inputs = np.array([\n",
        "    # X1, X2\n",
        "    [0, 0],\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [1, 1],\n",
        "])\n",
        "\n",
        "# Etiquetas objetivo (Lo que queremos [1])\n",
        "targets = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Lista para almacenar las métricas de cada época\n",
        "epoch_metrics = []\n",
        "\n",
        "# Entrenar el perceptrón\n",
        "metrics = train_perceptron(inputs, targets, epochs=6)\n",
        "\n",
        "# Imprimir los pesos finales y las métricas de cada época\n",
        "weights, metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtJiuaMAVzbo",
        "outputId": "00cd252f-e2a4-4180-cd6e-0acf188b9908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1 - Error: 3, Precisión: 0.25\n",
            "Época 2 - Error: 2, Precisión: 0.5\n",
            "Época 3 - Error: 3, Precisión: 0.25\n",
            "Época 4 - Error: 2, Precisión: 0.5\n",
            "Época 5 - Error: 1, Precisión: 0.75\n",
            "Época 6 - Error: 0, Precisión: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-1.5,  0.5,  1. ]),\n",
              " [(1, 3, 0.25),\n",
              "  (2, 2, 0.5),\n",
              "  (3, 3, 0.25),\n",
              "  (4, 2, 0.5),\n",
              "  (5, 1, 0.75),\n",
              "  (6, 0, 1.0)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tipos de Redes Neuronales"
      ],
      "metadata": {
        "id": "N9asZg1qZAa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos de los tipos más comunes de redes neuronales y sus diferencias principales. Cada tipo de red neuronal es adecuado para diferentes tipos de problemas, basándose en la naturaleza de los datos y el tipo de tarea a realizar. Por ejemplo, las CNNs son ideales para datos visuales, mientras que las RNN y sus variantes son mejores para datos secuenciales como texto o series temporales.\n",
        "\n",
        "1. **Redes Neuronales Densas o Completamente Conectadas (Fully Connected Neural Networks)**\n",
        "   - **Descripción**: En estas redes, cada neurona en una capa está conectada a todas las neuronas en la capa siguiente.\n",
        "   - **Uso**: Son útiles para problemas de clasificación y regresión general, donde la relación entre las entradas y las salidas no depende de datos espaciales o temporales.\n",
        "   - **Diferencia**: No consideran la relación espacial o temporal entre las entradas.\n",
        "\n",
        "2. **Redes Neuronales Convolucionales (Convolutional Neural Networks, CNNs)**\n",
        "   - **Descripción**: Se especializan en procesar datos con una estructura en forma de rejilla, como imágenes.\n",
        "   - **Uso**: Son ideales para tareas de visión computacional como reconocimiento de imágenes, análisis de video y clasificación de imágenes.\n",
        "   - **Diferencia**: Utilizan capas convolucionales que aplican filtros para capturar características espaciales.\n",
        "\n",
        "3. **Redes Neuronales Recurrentes (Recurrent Neural Networks, RNNs)**\n",
        "   - **Descripción**: Diseñadas para trabajar con secuencias de datos, tienen conexiones recurrentes que permiten información persistente.\n",
        "   - **Uso**: Se utilizan para tareas como el procesamiento del lenguaje natural, reconocimiento de voz y análisis de series temporales.\n",
        "   - **Diferencia**: Capaces de manejar secuencias de datos y recordar información previa gracias a sus conexiones recurrentes.\n",
        "\n",
        "4. **Long Short-Term Memory Networks (LSTM)**\n",
        "   - **Descripción**: Una variante avanzada de las RNN, diseñada para evitar el problema del desvanecimiento del gradiente al procesar secuencias largas.\n",
        "   - **Uso**: Útiles en aplicaciones similares a las RNN, pero mejoran el rendimiento en secuencias largas.\n",
        "   - **Diferencia**: Incorporan celdas especiales que pueden mantener información durante períodos más largos y regular el flujo de información.\n",
        "\n",
        "5. **Redes Neuronales de Memoria a Corto Plazo (Short-Term Memory Networks, GRU)**\n",
        "   - **Descripción**: Similar a LSTM, pero con una estructura más simplificada.\n",
        "   - **Uso**: Procesamiento de secuencias donde se requiere eficiencia computacional.\n",
        "   - **Diferencia**: Ofrecen un balance entre el poder de modelado de LSTM y la eficiencia de las RNN tradicionales.\n",
        "\n",
        "6. **Autoencoders**\n",
        "   - **Descripción**: Redes diseñadas para aprender representaciones codificadas (comprimidas) de los datos de entrada.\n",
        "   - **Uso**: Utilizados para reducción de dimensionalidad, des-ruido de datos y generación de nuevas instancias de datos.\n",
        "   - **Diferencia**: Su objetivo es reproducir la entrada en la salida, aprendiendo una representación compacta en el proceso.\n",
        "\n",
        "7. **Redes Neuronales Generativas Antagónicas (Generative Adversarial Networks, GANs)**\n",
        "   - **Descripción**: Consisten en dos redes, una generativa y otra discriminativa, que se entrenan en conjunto.\n",
        "   - **Uso**: Creación de contenido nuevo (como imágenes, música, texto), mejoramiento de imágenes, y más.\n",
        "   - **Diferencia**: Utilizan un enfoque de aprendizaje adversario donde dos redes se entrenan en competición.\n",
        "\n",
        "8. **Redes Neuronales de Gráficos (Graph Neural Networks, GNNs)**\n",
        "   - **Descripción**: Diseñadas para trabajar con datos en forma de gráficos.\n",
        "   - **Uso**: Análisis de redes sociales, sistemas de recomendación, y análisis de estructuras moleculares.\n",
        "   - **Diferencia**: Capaces de manejar datos estructurados como gráficos, donde las relaciones entre los nodos son clave."
      ],
      "metadata": {
        "id": "QGaiPQZ3ZfKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformers**\n",
        "\n",
        "Los **Transformers** representan un tipo avanzado de arquitectura de red neuronal que ha revolucionado particularmente el campo del procesamiento del lenguaje natural (PLN), aunque también se han aplicado en otras áreas como la visión por computadora.\n",
        "\n",
        "### Características Principales de los Transformers\n",
        "\n",
        "1. **Mecanismo de Atención**:\n",
        "   - Los Transformers se basan en un mecanismo de atención, específicamente la \"atención auto-dirigida\", que les permite ponderar la importancia de diferentes partes de la entrada de manera dinámica. Esto les permite centrarse en las partes más relevantes de los datos para una tarea específica.\n",
        "\n",
        "2. **Paralelismo y Eficiencia**:\n",
        "   - A diferencia de las RNN y LSTM, los Transformers no procesan los datos secuencialmente, lo que permite un mayor paralelismo en el entrenamiento y la inferencia. Esto los hace particularmente eficientes en el procesamiento de grandes conjuntos de datos.\n",
        "\n",
        "3. **Flexibilidad y Generalización**:\n",
        "   - Los Transformers han demostrado ser increíblemente flexibles y eficaces en una amplia gama de tareas de PLN, como la traducción automática, la generación de texto, la comprensión de lectura, y más. Su capacidad para manejar contextos largos y su eficiencia en capturar relaciones complejas los hace adecuados para tareas de PLN avanzadas.\n",
        "\n",
        "4. **Arquitectura de Encoder-Decoder**:\n",
        "   - Muchos modelos basados en Transformers utilizan una arquitectura de encoder-decoder. El encoder procesa la entrada y el decoder genera la salida. Esta estructura es especialmente útil en tareas como la traducción automática, donde la entrada y la salida son secuencias de diferentes longitudes.\n",
        "\n",
        "5. **Escalabilidad y Modelos Preentrenados**:\n",
        "   - Los Transformers escalan muy bien con grandes cantidades de datos y recursos computacionales, lo que ha llevado al desarrollo de modelos preentrenados de gran tamaño (como GPT-3, BERT, y otros) que pueden ser afinados para tareas específicas con resultados impresionantes.\n",
        "\n",
        "### Diferencias con Otras Redes Neuronales\n",
        "\n",
        "- **Atención vs. Recurrencia/Convolución**: Mientras que las RNN y las CNN se basan en la recurrencia y las convoluciones respectivamente, los Transformers dependen principalmente del mecanismo de atención para procesar secuencias de datos.\n",
        "- **Procesamiento Paralelo**: A diferencia de las RNN, que procesan secuencias de manera secuencial, los Transformers pueden procesar todos los elementos de una secuencia en paralelo, lo que los hace más rápidos y eficientes para ciertas aplicaciones, especialmente en hardware moderno.\n",
        "- **Capacidad de Manejar Contextos Largos**: Los Transformers son especialmente buenos en manejar contextos largos, lo que los hace adecuados para tareas de PLN donde la comprensión del contexto extenso es crucial.\n",
        "\n",
        "### Aplicaciones\n",
        "\n",
        "- **Procesamiento del Lenguaje Natural**: Traducción automática, generación de texto, respuesta a preguntas, resumen de texto, y más.\n",
        "- **Visión por Computadora**: Recientemente, los Transformers también han encontrado aplicaciones en el análisis de imágenes y videos, donde se utilizan para tareas como el reconocimiento de objetos y la segmentación de imágenes.\n",
        "\n",
        "Los Transformers han marcado un antes y un después en el campo de la inteligencia artificial, especialmente en PLN, debido a su capacidad para manejar eficientemente grandes cantidades de datos y capturar relaciones complejas en los mismos."
      ],
      "metadata": {
        "id": "7OPBwcPCaQFa"
      }
    }
  ]
}